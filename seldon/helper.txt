# build docker image
docker build --no-cache -t my-model:0.1 .
eval $(minikube -p minikube docker-env)
minikube docker-env  

# start from .yaml
kubectl create -f seldon-deploy.yaml

# forward port (just to not use istio)
kubectl port-forward service/seldon-model-example 8000:8000

# logs
kubectl logs deployment.apps/seldon-model-example-0-my-model -c seldon-container-engine
kubectl logs deployment.apps/seldon-model-example-0-my-model -c my-model

# delete
kubectl delete -f seldon-deploy.yaml

# set python home
export PATH="$(pyenv root)/shims:$PATH"

# test
python3 my-model-client.py http://localhost:8000/api/v0.1/predictions Abissinian_1.jpg



seldon-container-engine:

The service orchestrator is a component that is added to your inference graph to:

Correctly manage the request/response paths described by your inference graph
Expose Prometheus metrics
Provide Tracing via Open Tracing
Add CloudEvent based payload logging


Seldon Core provides an example Helm analytics chart that displays the above Prometheus metrics in Grafana. You can install it with:

helm install seldon-core-analytics seldon-core-analytics \
   --repo https://storage.googleapis.com/seldon-charts \
   --namespace seldon-system
   
   
Metrics doc:
https://docs.seldon.io/projects/seldon-core/en/v1.1.0/analytics/analytics.html